import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score


file_path = "winequality-red.csv"  
df = pd.read_csv(file_path, delimiter=';')

data = df[['fixed acidity', 'residual sugar']]
print(data.head())

labels = df['density']
print(labels[0:5]) # print the first 5 labels 

X = data
y = np.array(labels)

# remove nan (empty) values in the labels 
ynan = np.isnan(y)

not_ynan = [not y for y in ynan] # flip truth values for masking
X = X[not_ynan]
y = y[not_ynan]
X.reset_index(drop=True,inplace=True)

# split into 5 folds 
kf = KFold(n_splits=5)

beta = []
RMSE_train = []
RMSE_test = []
R2_train = []
R2_test = [] 

for i, (train_index, test_index) in enumerate(kf.split(X)):

    # define the training and testing sets 
    xTrain = X.loc[train_index,:]
    xTest = X.loc[test_index,:]
    yTrain = y[train_index]
    yTest = y[test_index]

    # replace nans in x with median from x_train 
    med_val = xTrain.median()
    xTrain = xTrain.fillna(med_val)
    xTest = xTest.fillna(med_val) # fill nans with the median from the training set, not the testing set 

    # standardize
    mean_val = xTrain.mean()
    std_val = xTrain.std()
    xTrain = (xTrain - mean_val) / std_val
    xTest = (xTest - mean_val) / std_val # use mean and std from the training set, not the testing set 

    # Create linear regression object
    regr = linear_model.LinearRegression()

    # Train the model using the training sets
    regr.fit(xTrain, yTrain)

    y_pred_train = regr.predict(xTrain) # find predicted y-values for the training set 
    y_pred_test = regr.predict(xTest) # find predicted y-values for the testing set 

 # this is going to create a colorful figure where every color is the data pairs from one fold
    plt.scatter(yTest,y_pred_test)
    plt.xlabel('True Y Values')
    plt.ylabel('Predicted Y Values')
    plt.savefig('y_pred_vs_y_true.png')
   

    # save the regression coefficients (beta)
    beta.append(regr.coef_)
    # print("Coefficients (Beta): \n", regr.coef_)

    # save the root mean squared error (RMSE): 0 is a perfect prediction
    this_rmse_train = mean_squared_error(yTrain, y_pred_train)**0.5 
    this_rmse_test = mean_squared_error(yTest, y_pred_test)**0.5 
    RMSE_train.append(this_rmse_train)
    RMSE_test.append(this_rmse_test)
    # print("Mean squared error: %.2f" % mean_squared_error(yTest, y_pred_test))

    # The coefficient of determination (R2): 1 is perfect prediction
    R2_train.append(r2_score(yTrain,y_pred_train))
    R2_test.append(r2_score(yTest,y_pred_test))
    # print("Coefficient of determination: %.2f" % r2_score(yTest, y_pred_test))

print("\nthis is the beta: ")
print(beta)
print("this is beta avg: ")
print(sum(beta)/len(beta))

print("\nthis is the RMSE for the training set: ")
print(RMSE_train)
print("this is RMSE train avg: ")
print(sum(RMSE_train)/len(RMSE_train))

print("\nthis is the RMSE for the testing set: ")
print(RMSE_test)
print("this is RMSE test avg: ")
print(sum(RMSE_test)/len(RMSE_test))

print("\nthis is the R2 for the training set: ")
print(R2_train)
print("this is R2 train avg: ")
print(sum(R2_train)/len(R2_train))

print("\nthis is the R2 for the testing set: ")
print(R2_test)
print("this is R2 test avg: ")
print(sum(R2_test)/len(R2_test))

# Plot outputs
plt.clf
plt.subplot(121)
plt.scatter(range(1,6), RMSE_train, color="black")
plt.scatter(range(1,6), RMSE_test, color="blue")

plt.xticks((1,2,3,4,5))

plt.xlabel('fold number')
plt.ylabel('RMSE')

plt.subplot(122)
plt.scatter(range(1,6), R2_train, color="black")
plt.scatter(range(1,6), R2_test, color="blue")

plt.xticks((1,2,3,4,5))

plt.xlabel('fold number')
plt.ylabel('R2')

# plt.show() 
plt.savefig('ml_results.png')
